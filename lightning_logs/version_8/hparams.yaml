policy_kwargs:
  num_encoder_layers: 6
  normalization: instance
  use_graph_context: false
  moe_kwargs:
    encoder:
      hidden_act: ReLU
      num_experts: 4
      k: 2
      noisy_gating: true
    decoder:
      light_version: false
      num_experts: 4
      k: 2
      noisy_gating: true
baseline: shared
num_augment: 8
augment_fn: dihedral8
first_aug_identity: true
feats: null
num_starts: null
batch_size: 128
train_data_size: 10000
val_batch_size: 100
val_data_size: 1000
optimizer: Adam
optimizer_kwargs:
  lr: 0.0001
  weight_decay: 1.0e-06
lr_scheduler: MultiStepLR
lr_scheduler_kwargs:
  milestones:
  - 451
  gamma: 0.1
moe_kwargs:
  encoder:
    hidden_act: ReLU
    num_experts: 4
    k: 2
    noisy_gating: true
  decoder:
    light_version: false
    num_experts: 4
    k: 2
    noisy_gating: true
test_batch_size: null
test_data_size: 10000
lr_scheduler_interval: epoch
lr_scheduler_monitor: val/reward
generate_default_data: false
shuffle_train_dataloader: false
dataloader_num_workers: 0
data_dir: data/
log_on_step: true
metrics: {}
baseline_kwargs: {}
reward_scale: null
